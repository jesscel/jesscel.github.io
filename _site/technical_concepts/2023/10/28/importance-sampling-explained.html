<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Importance Sampling Explained End-to-End | Jessica Liu‚Äôs Personal Blog ü¶≠</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Importance Sampling Explained End-to-End" />
<meta name="author" content="Jessica Liu" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Importance sampling is a useful technique when it‚Äôs infeasible for us to sample from the real distribution p, when we want to reduce variance of the current Monte Carlo estimator, or when we only know p up to a multiplicative constant." />
<meta property="og:description" content="Importance sampling is a useful technique when it‚Äôs infeasible for us to sample from the real distribution p, when we want to reduce variance of the current Monte Carlo estimator, or when we only know p up to a multiplicative constant." />
<link rel="canonical" href="http://localhost:4000/technical_concepts/2023/10/28/importance-sampling-explained.html" />
<meta property="og:url" content="http://localhost:4000/technical_concepts/2023/10/28/importance-sampling-explained.html" />
<meta property="og:site_name" content="Jessica Liu‚Äôs Personal Blog ü¶≠" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-10-28T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Importance Sampling Explained End-to-End" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Jessica Liu"},"dateModified":"2023-10-28T00:00:00-07:00","datePublished":"2023-10-28T00:00:00-07:00","description":"Importance sampling is a useful technique when it‚Äôs infeasible for us to sample from the real distribution p, when we want to reduce variance of the current Monte Carlo estimator, or when we only know p up to a multiplicative constant.","headline":"Importance Sampling Explained End-to-End","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/technical_concepts/2023/10/28/importance-sampling-explained.html"},"url":"http://localhost:4000/technical_concepts/2023/10/28/importance-sampling-explained.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Jessica Liu&apos;s Personal Blog ü¶≠" />

<link rel="shortcut icon" type="image/png" href="/assets/favicon.png"></head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Jessica Liu&#39;s Personal Blog ü¶≠</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/experiences/">Experiences</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Importance Sampling Explained End-to-End</h1>
    <p class="post-meta"><time class="dt-published" datetime="2023-10-28T00:00:00-07:00" itemprop="datePublished">
        Oct 28, 2023
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Importance sampling is a useful technique when it‚Äôs infeasible for us to sample from the real distribution p, when we want to reduce variance of the current Monte Carlo estimator, or when we only know p up to a multiplicative constant.</p>

<p>I found it confusing when I first learned about importance sampling because it applies to so many different scenarios and there are few resources online that walk us through the end-to-end derivation and reasoning process.</p>

<p>In this post, I will try my best to provide such an explanation, starting from the Monte Carlo Methods and ending with an example walk-through.</p>

<h1 id="monte-carlomethods">Monte Carlo¬†Methods</h1>
<p>Calculating expectation is an important task in machine learning. Mathematically, given an random variable x with probability density p(x), the expectation of a function of interest f(x) is computed as:</p>

<p>At a high level, expectation can be understood as the probability weighted average of f(x) over the entire space where x lives. When x is discrete, it becomes:</p>

<p>However, when x has high dimension, its probability space becomes exponentially large, making it infeasible to directly calculate the expectation using the integral above. Monte Carlo Methods address this problem by sampling from the probability distribution of x and estimating the expectation from the samples using this formula:
It turns out that this estimation approaches the real expectation as N is large enough.</p>

<h1 id="bias-and-variance-of-an-estimator">Bias and Variance of an Estimator</h1>

<p>According to Wikipedia, ‚Äúan estimator is a rule for calculating an estimate of a given quantity based on observed data‚Äù. Hence, the process of running the Monte Carlo method yields an estimator for the expectation.</p>

<p>It is important to distinguish between an estimator and a trial of the Monte Carlo method. We can think of an estimator as the aggregate result from running multiple trials of the Monte Carlo method. Due to the randomness in the sampling step, every time we run through the two-step process of sampling and estimating, we produce a slightly different estimation, which we call s, for the expectation. And the estimations themselves have a distribution, which has its own expectation and variance.</p>

<p>The Central Limit Theorem (CLT) states that
‚Äú‚Ä¶for independent and identically distributed random variables, the sampling distribution of the standardized sample mean tends towards the standard normal distribution even if the original variables themselves are not normally distributed‚Äù‚Ää-‚ÄäWikipedia
Translating that into our example, we can say that when N is large enough, the distribution of s is a normal distribution even though p(x) is not normally distributed.
Since Monte Carlo Method generates unbiased estimator for the expectation of a distribution, the expectation and variance of the estimator are computed as follows:</p>

<h1 id="importance-samplingis">Importance Sampling¬†(IS)</h1>

<h2 id="how-does-iswork">How does IS¬†work?</h2>
<p>Importance sampling introduces a new distribution q(x), allowing us to get a potentially better estimation of the expectation f(x). Specifically, we can rewrite the expectation formula as follows:</p>

<p>The equation went through the following procedures:</p>

<p>The integral is multiplied by q(x)/q(x), which is equal to 1 and doesn‚Äôt break anything. The only requirement is that we need to have q(x)&gt;0 whenever p(x) is nonzero. This is because if q(x) is zero when p(x) is nonzero, the equation will not hold as some values of x will be missing from the integral.</p>

<p>Then, we reorder the terms and group f(x)(p(x)/q(x)) as the new function of interest whose expectation we are computing. This allows us to sample from a different distribution q(x) and still get an estimation of the same value‚Ää-‚Ääi.e. the expectation of f(x) over p(x)‚Ää-‚Ääas before.</p>

<h2 id="why-does-ishelp">Why does IS¬†help?</h2>
<p>In the simple term, IS helps because it 1) yields an unbiased estimator and 2) helps lower the variance of the estimator.</p>

<p>Say we use IS and estimate the expectation below:</p>

<p>We can compute the expectation and variance of the new estimator r as follows:</p>

<p>We see that the new estimator r is still an unbiased estimator for the expectation of f(x) since the expectation of r is equal to the value being estimated.</p>

<p>Now that we have an unbiased estimator, let‚Äôs look at the variance. In reality, we might not be able to run the Monte Carlo method a large number of times due to cost constraint. Hence, a lower variance for the estimator indicates better quality. We can pick q(x) such that it results in lower variance; that is,</p>

<table>
  <tbody>
    <tr>
      <td>To achieve this, we want q(x) to be large where</td>
      <td>p(x)f(x)</td>
      <td>is high.</td>
    </tr>
  </tbody>
</table>

<p>To understand why such q(x) helps reduce the variance, think about a case where f(x) has very uneven distribution while p(x) is a uniform distribution. Also assume that we have a very limited budget and it‚Äôs infeasible for us to sample a large amount of examples or run many Monte Carlo trials.</p>

<p>As we see in the left picture, f(x) has a very small region that contributes the most to the expectation. When we cannot sample a large amount from p(x), it‚Äôs likely that we miss that small ‚Äúimportant‚Äù region of f(x), resulting in a bad estimation.</p>

<p>Illustration of the effect of importance samplingIf we pick q(x) such that it allows us to sample from the ‚Äúimportant‚Äù region with higher probability, we see that the resulting ratio p(x)/q(x) that we multiply f(x) will effectively unweight the ‚Äúimportant‚Äù regions that we put a higher probability into. This unweighting ratio is what makes the IS estimator unbiased and lowers the variance.</p>

<h2 id="example">Example</h2>
<p>To see how IS reduces variance, let‚Äôs look at an example with a discrete random variable X that can take on integer values in range [1,5]. Say p(X) is uniform over all values of X, so we have p(x) = 1/5.</p>

<p>Let‚Äôs define the function of interest f(x) as follows:</p>

<p>We can compute the expectation and variance of the Monte Carlo estimator over p(x) as follows:</p>

<p>Now say we have a new distribution q(x) defined as follows:</p>

<p>Now, we can compute the expectation and variance of our estimator as follows:</p>

<p>p(x) and¬†q(x)We see that the variance given by the IS estimator is much lower than the one given by the uniform estimator.</p>

<h1 id="references">References</h1>
<ul>
  <li><a href="https://www.youtube.com/watch?v=C3p2wI4RAi8&amp;t=437s">üì∫ Importance Sampling by Mutual Information</a></li>
  <li><a href="https://www.statlect.com/asymptotic-theory/importance-sampling">üìù Importance sampling by Marco Taboga</a></li>
</ul>

<hr />

<p>I hope you enjoyed this article! Connect with me on LinkedIn if you are also interested in AI, ML, databases, and more.</p>

  </div><a class="u-url" href="/technical_concepts/2023/10/28/importance-sampling-explained.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Jessica Liu</li>
          <li><a class="u-email" href="mailto:liuec.jessica2000@gmail.com">liuec.jessica2000@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>Writing about my learning journey.
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="https://www.linkedin.com/in/jekyll" target="_blank" title="linkedin">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#linkedin"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="https://x.com/jekyllrb" target="_blank" title="x">
    <svg class="svg-icon grey">
      <use xlink:href="/assets/minima-social-icons.svg#x"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

</html>
